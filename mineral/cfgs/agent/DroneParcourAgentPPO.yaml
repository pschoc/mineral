seed: ${..seed}
algo: PPO

print_every: 1
ckpt_every: 200

tracker_len: 100
metrics_kwargs:
  save_video_every: 0
  save_video_consecutive: 0
  # info_keys:
  #   scalar: 'consecutive_successes'

network:
  normalize_input: True
  actor_critic: ActorCritic
  actor_critic_kwargs:
    separate_value_mlp: True
    mlp_kwargs:
      units: [512, 256, 64]

    #   norm_type: null
    #   act_type: ELU
    # actor_dist_kwargs: {dist_type: normal}
    # fixed_sigma: True

      norm_type: LayerNorm
      act_type: SiLU
    actor_dist_kwargs: {dist_type: dreamerv3_normal, minstd: 0.1, maxstd: 1.0}
    fixed_sigma: False

ppo:
  multi_gpu: ${...multi_gpu}
  num_actors: ${...task.env.numEnvs}
  normalize_value: False
  value_bootstrap: True
  reward_shaper:
    fn: scale
    scale: 0.01
  clip_value_loss: False
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  optim_type: Adam
  optim_kwargs:
    lr: 5e-3
    eps: 1e-3
  lr_schedule: kl  # 'fixed' | 'linear' | 'kl' | 'cos'
  kl_threshold: 0.016
  entropy_coef: 0.0
  e_clip: 0.2
  use_smooth_clamp: False
  critic_coef: 4
  bounds_loss_coef: 0.0001
  bounds_type: bound
  max_grad_norm: 1.0
  truncate_grads: True
  horizon_len: 1000
  minibatch_size: 500
  mini_epochs: 5
  max_agent_steps: 700e6
