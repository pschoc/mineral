seed: ${..seed}
algo: PPO

info_keys_scalar: 'consecutive_successes'

network:
  mlp:
    units: [512, 256, 128]
  separate_value_mlp: True

  # act_type: ELU
  # norm_type: null
  # actor_dist:
  #   dist: normal

  act_type: SiLU
  norm_type: LayerNorm
  actor_dist:
    dist: normal_dv3
    minstd: 0.1
    maxstd: 1.0

ppo:
  name: ${resolve_default:AllegroHand,${...experiment}}
  multi_gpu: ${...multi_gpu}
  num_actors: ${...task.env.numEnvs}
  normalize_input: True
  normalize_value: True
  value_bootstrap: True
  reward_shaper:
    fn: scale
    scale: 0.01
  clip_value_loss: False
  normalize_advantage: True
  gamma: 0.99
  tau: 0.95
  learning_rate: 5e-4
  adam_eps: 1e-5
  lr_schedule: kl  # 'fixed' | 'linear' | 'kl' | 'cos'
  kl_threshold: 0.016
  save_best_after: 500
  save_frequency: 200
  save_video_every: 0
  save_video_consecutive: 0
  grad_norm: 1.0
  entropy_coef: 0.0
  truncate_grads: True
  e_clip: 0.2
  use_smooth_clamp: False
  horizon_length: 8
  minibatch_size: 32768
  mini_epochs: 5
  critic_coef: 4
  bounds_loss_coef: 0.0001
  bounds_type: bound
  max_agent_steps: 700000000
